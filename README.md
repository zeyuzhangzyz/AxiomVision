# AxiomVision: Accuracy-Guaranteed Adaptive Visual Model Selection for Perspective-Aware Video Analytics

This repository contains the source code for reproducing the results of our paper titled [**AxiomVision: Accuracy-Guaranteed Adaptive Visual Model Selection for Perspective-Aware Video Analytics**](https://arxiv.org/abs/2407.20124), which is accepted by ACM MM 2024.

![](design.png)

AxiomVision is a general online streaming framework that adaptively selects the most effective visual model based on the end-edge-cloud architecture to ensure the accuracy of video stream analysis in diverse scenarios.

## ðŸ”§Prerequisites

(If you're only interested in the plotting part, please ignore this section.)

**If you want to run based on your dataset**

To get started, ensure that you can run multiple target detection models (such as [YOLOv5](https://github.com/ultralytics/yolov5), [SSD](https://github.com/lufficc/SSD), etc.) or other computer vision tasks, and importantly, modify their source code to ensure that the output TXT detection results have the same format.

For example, in the output TXT file of YOLOv5, if confidence is included, the format is as follows: the first column is the class label, the second is the normalized center x-coordinate, the third is the normalized center y-coordinate, the fourth is the normalized width, the fifth is the normalized height, and the sixth is the confidence score.  In contrast, [mmdetection](https://github.com/open-mmlab/mmdetection)'s format outputs a JSON file for each image frame. This JSON file contains a dictionary with three keys: labels, scores, and bounding boxes. The values of boxes are the coordinates before normalization and sequentially represent the x and y coordinates of the top-left corner, followed by the x and y coordinates of the bottom-right corner.

We modified the source code of these programs to standardize the output file format by altering the way the results are saved. 


We also used the [Paddle](https://github.com/PaddlePaddle/PaddleSeg) segmentation model and created an environment named "Paddle" You only need to prepare this setup if you want to test segmentation tasks.

## ðŸ“š Dataset: 
We searched for online traffic cameras on YouTube and found four different views monitoring the same intersection. We saved scenes from various environments, including daytime, dusk, night, and snowy conditions.



|  Source  | Type    | URL                                                         |
|---------|---------|-------------------------------------------------------------|
| Youtube | Videos | https://www.youtube.com/watch?v=1EiC9bvVGnk |
| Youtube | Videos | https://www.youtube.com/watch?v=FmoclK_hKz8 |
| Youtube | Videos | https://www.youtube.com/watch?v=BN7gzH-i-zo |
| Youtube | Videos | https://www.youtube.com/watch?v=Zj0pXlq2-jI |
| Github | Videos | https://github.com/sjtu-medialab/Free-Viewpoint-RGB-D-Video-Dataset |
| CADC  Dataset |   Videos  | http://cadcd.uwaterloo.ca/ |
| COCO Dataset | Images  | http://images.cocodataset.org/zips/train2017.zip |


## ðŸ‘‰ Code Structure

```
.
â”œâ”€â”€ README.md
â”œâ”€â”€ Motivation
â”‚Â Â  â”œâ”€â”€ Base
â”‚Â Â  â”‚   â”œâ”€â”€ video.py
â”‚Â Â  â”‚   â””â”€â”€ performance.py
â”‚Â Â  â”œâ”€â”€ data
â”‚Â Â  â”‚   â”œâ”€â”€ figure1_data.npy
â”‚Â Â  â”‚   â”œâ”€â”€ ...
â”‚Â Â  â”‚   â””â”€â”€ figure6_data_segmentation.npy
â”‚Â Â  â”œâ”€â”€ figure4_src_img
â”‚Â Â  â”‚   â”œâ”€â”€ light1.jpg
â”‚Â Â  â”‚   â”œâ”€â”€ ...
â”‚Â Â  â”‚   â””â”€â”€ light4.jpg
â”‚Â Â  â”œâ”€â”€ figure5_src_img
â”‚Â Â  â”‚   â”œâ”€â”€ perspective_1.jpg
â”‚Â Â  â”‚   â”œâ”€â”€ ...
â”‚Â Â  â”‚   â””â”€â”€ perspective_4.jpg
â”‚Â Â  â”œâ”€â”€ motivation_config.json
â”‚Â Â  â”œâ”€â”€ motivation_dataprocess.py
â”‚Â Â  â””â”€â”€ motivation_plot.py
â””â”€â”€ Experiment
 Â Â  â”œâ”€â”€ Algorithm
 Â Â  â”‚   â”œâ”€â”€ Axiomvision.py
 Â Â  â”‚   â”œâ”€â”€ wo_Group.py
Â Â   â”‚   â”œâ”€â”€ ... (different algorithms)
Â Â   â”‚   â”œâ”€â”€ Greedy.py
Â Â   â”‚   â”œâ”€â”€ paras.py
    â”‚   â”œâ”€â”€ experiment_config.json
Â Â   â”‚   â”œâ”€â”€ performance.py
    â”‚   â”œâ”€â”€ video.py
Â Â   â”‚   â”œâ”€â”€ Base.py
 Â Â  â”‚   â””â”€â”€ Enviroment.py
 Â Â  â”œâ”€â”€ retrained_models
 Â Â  â”‚   â”œâ”€â”€ coco5k_Significantly_Darker_100
 Â Â  â”‚   â”œâ”€â”€ ...
 Â Â  â”‚   â””â”€â”€ coco5k_Extremely_Bright_100
 Â Â  â”œâ”€â”€ main.py
    â”œâ”€â”€ experiment_prepare_config.json
    â”œâ”€â”€ experiment_dataprocess.py
    â”œâ”€â”€ demo.py (This is an example of modifying different model's source code to analyze both time and accuracy together.)
 Â Â  â””â”€â”€ svd_decomposition.py

```

## ðŸš€How to Run

Execute the following command to reproduce the figures in the paper:

####  Motivation:

1. Set up your environment
2. Prepare your dataset (if needed)
3. Run `motivation_plot.py` (activate the function you want to run)
4. Run `motivation_dataprocess.py` (activate the function you want to run)


####  Experiment:

1. Run `experiment_dataprocess.py` (activate the function you want to run)
2. Run `svd_decomposition.py`
3. Run `main.py` (activate the function you want to run)

To compare various algorithms, please consider three dimensions: algorithm_time, average_payoff, and dnn_time.

For details on obtaining inference time, please refer to `demo.py`. The algorithm time can be recorded by writing to a txt file within the activation function.

## ðŸŒŸ Citation

If you find this work helpful to your research, please kindly consider citing our paper.

```
@inproceedings{dai2024axiomvision,
  title={AxiomVision: Accuracy-Guaranteed Adaptive Visual Model Selection for Perspective-Aware Video Analytics},
  author={Dai, Xiangxiang and Zhang, Zeyu and Yang, Peng and Xu, Yuedong and Liu, Xutong and Lui, John CS},
  booktitle={Proceedings of the 32nd ACM International Conference on Multimedia},
  pages={7229--7238},
  year={2024}
}
```